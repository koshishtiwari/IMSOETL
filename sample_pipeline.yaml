# Sample Pipeline Configuration for IMSOETL
# This demonstrates a real data processing pipeline using multiple engines

name: "Sales Data Analysis Pipeline"
description: "Comprehensive sales data processing and analysis pipeline"
version: "1.0"

# Pipeline metadata
metadata:
  author: "IMSOETL"
  created: "2025-06-20"
  tags: ["sales", "analytics", "etl"]

# Data sources
sources:
  sales_data:
    type: "csv"
    path: "./demo_data/sales.csv"
    schema:
      - {name: "id", type: "int"}
      - {name: "product", type: "string"}
      - {name: "quantity", type: "int"}
      - {name: "price", type: "float"}
      - {name: "date", type: "date"}
      - {name: "customer_id", type: "int"}
      - {name: "region", type: "string"}

# Pipeline tasks
tasks:
  - task_id: "load_sales_data"
    task_name: "Load Sales Data"
    task_type: "data_load"
    source: 
      path: "./demo_data/sales.csv"
      format: "csv"
    options:
      header: true
      encoding: "utf-8"
    engine_hint: "pandas"
    
  - task_id: "clean_data"
    task_name: "Clean and Validate Data"
    task_type: "data_transformation"
    dependencies: ["load_sales_data"]
    source_data: "sales_data"
    transformation:
      type: "data_cleaning"
      operations:
        - {type: "remove_nulls", columns: ["id", "product", "date"]}
        - {type: "validate_range", column: "price", min: 0}
        - {type: "standardize_dates", column: "date", format: "YYYY-MM-DD"}
    engine_hint: "pandas"
    
  - task_id: "calculate_totals"
    task_name: "Calculate Sale Totals"
    task_type: "sql_query"
    dependencies: ["clean_data"]
    query: |
      SELECT 
        id,
        product,
        quantity,
        price,
        (quantity * price) as total_amount,
        date,
        customer_id,
        region
      FROM data
    engine_hint: "duckdb"
    
  - task_id: "regional_analysis"
    task_name: "Regional Sales Analysis"
    task_type: "sql_query"
    dependencies: ["calculate_totals"]
    query: |
      SELECT 
        region,
        COUNT(*) as total_orders,
        SUM(total_amount) as total_revenue,
        AVG(total_amount) as avg_order_value,
        SUM(quantity) as total_quantity
      FROM data 
      GROUP BY region
      ORDER BY total_revenue DESC
    engine_hint: "duckdb"
    
  - task_id: "product_analysis"
    task_name: "Product Performance Analysis"
    task_type: "sql_query"
    dependencies: ["calculate_totals"]
    query: |
      SELECT 
        product,
        COUNT(*) as order_count,
        SUM(quantity) as total_sold,
        SUM(total_amount) as revenue,
        AVG(price) as avg_price
      FROM data 
      GROUP BY product
      ORDER BY revenue DESC
      LIMIT 10
    engine_hint: "duckdb"
    
  - task_id: "monthly_trends"
    task_name: "Monthly Sales Trends"
    task_type: "sql_query"
    dependencies: ["calculate_totals"]
    query: |
      SELECT 
        strftime('%Y-%m', date) as month,
        COUNT(*) as orders,
        SUM(total_amount) as revenue,
        COUNT(DISTINCT customer_id) as unique_customers
      FROM data 
      GROUP BY strftime('%Y-%m', date)
      ORDER BY month
    engine_hint: "duckdb"
    
  - task_id: "export_results"
    task_name: "Export Analysis Results"
    task_type: "batch_processing"
    dependencies: ["regional_analysis", "product_analysis", "monthly_trends"]
    steps:
      - name: "export_regional"
        type: "save_data"
        source: "regional_analysis_result"
        target: "./output/regional_analysis.csv"
      - name: "export_products" 
        type: "save_data"
        source: "product_analysis_result"
        target: "./output/product_analysis.csv"
      - name: "export_trends"
        type: "save_data"
        source: "monthly_trends_result"
        target: "./output/monthly_trends.csv"
    engine_hint: "pandas"

# Execution configuration
execution:
  mode: "dag"  # Execute based on dependencies
  parallel_limit: 3
  timeout: 300  # 5 minutes
  retry_failed: true
  retry_count: 2
  
# Output configuration
outputs:
  - name: "summary_report"
    type: "json"
    path: "./output/pipeline_summary.json"
    include_metadata: true
    
  - name: "execution_log"
    type: "log"
    path: "./output/execution.log"
    level: "INFO"

# Engine preferences
engine_config:
  selection_strategy: "auto"
  fallback_enabled: true
  performance_monitoring: true
