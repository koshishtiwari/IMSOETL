# IMSOETL Configuration File

# Project settings
project:
  name: "IMSOETL"
  version: "0.1.0"
  description: "I'm so ETL - Agentic Data Engineering Platform"

# Agent Configuration
agents:
  orchestrator:
    enabled: true
    config:
      max_concurrent_sessions: 10
      session_timeout: 3600  # seconds
      
  discovery:
    enabled: true
    config:
      connection_timeout: 30
      max_parallel_discoveries: 5
      
  schema:
    enabled: true
    config:
      compatibility_threshold: 0.7
      auto_mapping: true
      
  transformation:
    enabled: true
    config:
      max_transformation_complexity: 10
      
  quality:
    enabled: true
    config:
      quality_threshold: 0.8
      auto_fix_issues: false
      
  execution:
    enabled: true
    config:
      max_parallel_jobs: 3
      job_timeout: 7200  # seconds
      
  monitoring:
    enabled: true
    config:
      metrics_retention_days: 30
      alert_channels: ["email", "slack"]

# Data Source Configurations
data_sources:
  mysql:
    type: "mysql"
    host: "${MYSQL_HOST:-localhost}"
    port: "${MYSQL_PORT:-3306}"
    database: "${MYSQL_DATABASE:-ecommerce}"
    username: "${MYSQL_USER:-root}"
    password: "${MYSQL_PASSWORD:-}"
    
  mysql_default:
    type: "mysql"
    host: "${MYSQL_HOST:-localhost}"
    port: "${MYSQL_PORT:-3306}"
    database: "${MYSQL_DATABASE:-ecommerce}"
    username: "${MYSQL_USER:-root}"
    password: "${MYSQL_PASSWORD:-}"
    
  postgres_default:
    type: "postgres"
    host: "localhost"
    port: 5432
    # credentials via environment variables
    # POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DATABASE
    
  snowflake_default:
    type: "snowflake"
    # credentials via environment variables
    # SNOWFLAKE_ACCOUNT, SNOWFLAKE_USER, SNOWFLAKE_PASSWORD
    # SNOWFLAKE_WAREHOUSE, SNOWFLAKE_DATABASE, SNOWFLAKE_SCHEMA

# Logging Configuration
logging:
  level: "INFO"
  format: "structured"
  handlers:
    - type: "console"
      level: "INFO"
    - type: "file"
      level: "DEBUG"
      filename: "logs/imsoetl.log"
      max_size: "10MB"
      backup_count: 5

# LLM Configuration
llm:
  ollama:
    enabled: true
    base_url: "http://localhost:11434"
    model: "gemma3:4b"
    timeout: 30
    
  gemini:
    enabled: true
    model: "gemini-1.5-flash"
    # API key via environment variable: GEMINI_API_KEY
    api_key: "${GEMINI_API_KEY}"

# Legacy AI Model Configuration (deprecated - use llm section above)
ai_models:
  primary:
    provider: "gemini"
    model: "gemini-pro"
    # API key via environment variable: GEMINI_API_KEY
    
  fallback:
    provider: "ollama"
    model: "gemma3:4b"
    endpoint: "http://localhost:11434"

# Pipeline Configuration
pipelines:
  default_batch_size: 10000
  max_retries: 3
  retry_delay: 60  # seconds
  checkpoint_frequency: 1000  # records

# Security Configuration
security:
  encryption:
    enabled: true
    algorithm: "AES-256"
  
  authentication:
    required: true
    method: "api_key"  # or "oauth", "jwt"
  
  audit_logging:
    enabled: true
    retention_days: 90

# Performance Configuration
performance:
  worker_threads: 4
  memory_limit: "2GB"
  disk_cache_size: "1GB"
  
# Monitoring and Alerts
monitoring:
  metrics:
    enabled: true
    collection_interval: 60  # seconds
    
  alerts:
    enabled: true
    error_threshold: 5  # errors per minute
    latency_threshold: 5000  # milliseconds
    
  health_checks:
    enabled: true
    interval: 30  # seconds

# Execution Engine Configuration
execution_engines:
  # Engine selection strategy
  selection_strategy: "auto"  # auto, performance, memory_efficient, distributed, manual
  
  # Primary engine preference
  primary_engine: "pandas"  # pandas, duckdb, spark
  
  # Data size thresholds for automatic engine selection
  small_data_threshold: 10000    # rows - use Pandas
  medium_data_threshold: 1000000 # rows - use DuckDB
  large_data_threshold: 10000000 # rows - use Spark
  
  # Engine-specific configurations
  pandas:
    enabled: true
    chunk_size: 10000
    memory_map: true
    
  duckdb:
    enabled: true
    memory_limit: "1GB"
    threads: 4
    temp_directory: "/tmp/duckdb"
    
  spark:
    enabled: false  # Enable when Spark cluster is available
    app_name: "IMSOETL-Spark"
    master: "local[*]"
    driver_memory: "2g"
    executor_memory: "1g"
    
  polars:
    enabled: false  # Future enhancement
    streaming: true
    lazy: true
